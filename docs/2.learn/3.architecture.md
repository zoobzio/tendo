---
title: Architecture
description: Internal design, memory layout, and device dispatch
author: zoobzio
published: 2025-12-16
updated: 2025-12-16
tags:
  - Architecture
  - Internals
  - Performance
---

# Architecture

Understanding tendo's internals helps you reason about performance, memory usage, and device management.

## Overview

```
┌─────────────────────────────────────────────────────────────────────┐
│                              Tendo                                   │
│                                                                      │
│  ┌────────────────────────────────────────────────────────────────┐  │
│  │                         Tensor                                  │  │
│  │                                                                 │  │
│  │   shape: []int      stride: []int      offset: int             │  │
│  │         │                  │                 │                  │  │
│  │         └──────────────────┴─────────────────┘                  │  │
│  │                            │                                    │  │
│  │                     ┌──────▼──────┐                             │  │
│  │                     │   Storage   │                             │  │
│  │                     └──────┬──────┘                             │  │
│  └────────────────────────────┼────────────────────────────────────┘  │
│                               │                                      │
│               ┌───────────────┴───────────────┐                      │
│               ▼                               ▼                      │
│   ┌─────────────────────┐         ┌─────────────────────┐           │
│   │    CPUStorage       │         │    CUDAStorage      │           │
│   │                     │         │                     │           │
│   │  data: []float32    │         │  ptr: uintptr       │           │
│   │  dtype: DType       │         │  device: int        │           │
│   │                     │         │  pool: *Pool        │           │
│   │  [Go heap memory]   │         │  [GPU memory]       │           │
│   └─────────────────────┘         └─────────────────────┘           │
│                                                                      │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │                          Pool                                │   │
│   │                                                              │   │
│   │   CPU blocks: map[sizeClass][]Storage                       │   │
│   │   CUDA blocks: map[device]map[sizeClass][]uintptr           │   │
│   │                                                              │   │
│   │   [Reuses allocations to reduce overhead]                   │   │
│   └─────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
```

## Memory Layout

### Row-Major (C-Order)

Tendo uses row-major ordering, where the last dimension varies fastest in memory:

```
Tensor shape [2, 3]:

Logical view:          Memory layout:
┌─────────────┐        ┌───┬───┬───┬───┬───┬───┐
│ 0   1   2   │        │ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │
│ 3   4   5   │        └───┴───┴───┴───┴───┴───┘
└─────────────┘          contiguous in memory
```

### Stride Calculation

For a contiguous tensor, strides are computed from shape:

```go
// Shape [2, 3, 4]
// Stride [12, 4, 1]
//
// stride[i] = product of shape[i+1:]
// stride[0] = 3 * 4 = 12
// stride[1] = 4
// stride[2] = 1
```

### Element Access

Element at index `[i, j, k]` is located at:

```
offset + i*stride[0] + j*stride[1] + k*stride[2]
```

This formula enables views without copying data.

## Views and Contiguity

### View Operations

Some operations create views sharing the underlying storage:

```go
ctx := context.Background()

t := tendo.Arange(0, 12, 1)  // [0, 1, 2, ..., 11]

// Reshape creates a view
r, _ := tendo.Reshape(3, 4).Run(ctx, t)
r.Storage() == t.Storage()  // true - same memory

// Transpose creates a view with modified strides
tr, _ := tendo.Transpose(0, 1).Run(ctx, r)
tr.Stride()  // [1, 4] - non-standard
```

### Contiguity

A tensor is contiguous when elements are laid out sequentially:

```go
ctx := context.Background()

t := tendo.Zeros(3, 4)
t.IsContiguous()  // true

tr, _ := tendo.Transpose(0, 1).Run(ctx, t)
tr.IsContiguous()  // false - strides don't match shape
```

Non-contiguous tensors work but may require copying:

```go
// Many operations check contiguity
func SomeOp(t *Tensor) *Tensor {
    if !t.IsContiguous() {
        t = t.Contiguous()  // Copy to contiguous layout
    }
    // ... operate on contiguous data
}
```

## Storage Backend

### Interface

```go
type Storage interface {
    Ptr() uintptr      // Raw pointer to data
    Size() int         // Bytes allocated
    Len() int          // Element count
    Device() Device    // CPU or CUDA
    DType() DType      // Float32, Float16, BFloat16
    Clone() Storage    // Deep copy
    Free()             // Release memory
}
```

### CPU Implementation

```go
type CPUStorage struct {
    data  []float32    // Go slice
    dtype DType
}

func (s *CPUStorage) Ptr() uintptr {
    return uintptr(unsafe.Pointer(&s.data[0]))
}

func (s *CPUStorage) Free() {
    s.data = nil  // Let GC handle it
}
```

CPU storage leverages Go's memory management. No explicit freeing required, but `Free()` releases the reference early.

### CUDA Implementation

```go
type CUDAStorage struct {
    ptr    uintptr    // Device pointer
    size   int        // Bytes
    len    int        // Elements
    device int        // GPU index
    dtype  DType
    pool   *Pool      // Optional pool reference
}

func (s *CUDAStorage) Free() {
    if s.pool != nil {
        s.pool.FreeCUDA(s.ptr, s.len, s.dtype, s.device)
    } else {
        cudaFree(s.ptr)
    }
}
```

CUDA storage requires explicit management. Pool integration enables memory reuse.

## Memory Pool

The pool reduces allocation overhead by reusing freed memory.

### Size Classes

Memory is bucketed by size class (powers of 2):

```go
func roundUpPow2(n int) int {
    // Round to next power of 2
    // 1000 -> 1024
    // 2000 -> 2048
}
```

This trades some memory waste for faster allocation matching.

### Pool Structure

```go
type Pool struct {
    cpu  *cpuPool                    // CPU memory blocks
    cuda map[int]*cudaPool           // Per-device GPU blocks
    stats PoolStats                  // Tracking
}

type cpuPool struct {
    blocks map[int][]Storage         // sizeClass -> available blocks
}

type cudaPool struct {
    device int
    blocks map[int][]uintptr         // sizeClass -> device pointers
}
```

### Allocation Flow

```
AllocCUDA(numel, dtype, device):
  1. Compute size class: roundUpPow2(numel)
  2. Check pool for cached block
     - Found: return cached pointer, update stats
     - Not found: cudaMalloc, update stats
  3. Return pointer

FreeCUDA(ptr, numel, dtype, device):
  1. Compute size class
  2. Add to pool's free list
  3. Update stats
```

### Pool Statistics

```go
type PoolStats struct {
    CPUAllocations   int64
    CPUDeallocations int64
    CPUBytesInUse    int64
    CPUBytesCached   int64

    CUDAAllocations   map[int]int64  // per device
    CUDADeallocations map[int]int64
    CUDABytesInUse    map[int]int64
    CUDABytesCached   map[int]int64
}

stats := pool.Stats()
fmt.Printf("GPU 0 bytes in use: %d\n", stats.CUDABytesInUse[0])
```

## Operation Dispatch

### CPU Operations

Most operations iterate over storage directly:

```go
func cpuAdd(a, b *CPUStorage, outShape []int) *CPUStorage {
    result := NewCPUStorage(Numel(outShape), a.DType())
    dataA, dataB := a.Data(), b.Data()
    dataOut := result.Data()

    for i := range dataOut {
        // Broadcasting handled via index computation
        dataOut[i] = dataA[idxA] + dataB[idxB]
    }
    return result
}
```

### CUDA Operations

CUDA operations use cuBLAS or custom kernels:

```go
func cudaMatMul(a, b, c *CUDAStorage, m, n, k int) error {
    // cuBLAS SGEMM: C = alpha*A*B + beta*C
    return cublasSgemm(
        handle,
        cublasOpN, cublasOpN,
        m, n, k,
        1.0,           // alpha
        a.Ptr(), m,
        b.Ptr(), k,
        0.0,           // beta
        c.Ptr(), m,
    )
}
```

### Device Dispatch

Operations check device and dispatch accordingly:

```go
func MatMul(a, b *Tensor) *Tensor {
    if a.Device() != b.Device() {
        panic(&DeviceMismatchError{A: a.Device(), B: b.Device()})
    }

    switch a.Device().Type {
    case CPU:
        return cpuMatMul(a, b)
    case CUDA:
        return cudaMatMul(a, b)
    }
}
```

## Event Emission

Operations emit capitan signals for observability:

```go
func MatMul(a, b *Tensor) *Tensor {
    result := // ... compute

    capitan.Emit(ctx, OpMatMul,
        KeyInputA.Field(a),
        KeyInputB.Field(b),
        KeyOutput.Field(result),
    )

    return result
}
```

### Signal Categories

| Category | Signals | Purpose |
|----------|---------|---------|
| Lifecycle | `TensorCreated`, `TensorFreed`, `TensorTransfer` | Track tensor allocation |
| Operations | `OpAdd`, `OpMatMul`, `OpReLU`, ... | Capture computation graph |
| Memory | `PoolAlloc`, `PoolFree` | Monitor memory usage |

### Debugging with Signals

Signals enable operation tracing:

```go
// Trace all operations
capitan.Observe(func(ctx context.Context, e *capitan.Event) {
    log.Printf("Op: %s", e.Signal.Name())
})

// Run inference
output := network.Forward(input)
```

## Concurrency

### Thread Safety

- Tensor creation is thread-safe
- Pool operations are mutex-protected
- Individual tensors are not synchronized

```go
// Safe: creating tensors concurrently
var wg sync.WaitGroup
for i := 0; i < 10; i++ {
    wg.Add(1)
    go func() {
        t := tendo.Zeros(100, 100)
        // use t...
        wg.Done()
    }()
}
wg.Wait()

// Unsafe: concurrent writes to same tensor
// Don't do this without external synchronization
```

### CUDA Streams

CUDA operations currently use the default stream. Operations on the same device are serialized. Call `Sync()` to wait for completion:

```go
gpu := tendo.CUDADevice(0)
t := tendo.ZerosOn(gpu, 1000, 1000)
// ... operations ...

if cuda, ok := t.Storage().(*CUDAStorage); ok {
    cuda.Sync()  // Wait for all operations to complete
}
```
