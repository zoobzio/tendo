---
title: Building Pipelines
description: Composing tensor operations with pipz
author: zoobzio
published: 2025-12-16
updated: 2025-12-16
tags:
  - Pipelines
  - Composition
  - pipz
---

# Building Pipelines

Tendo integrates with [pipz](https://github.com/zoobzio/pipz) for composable computation graphs. All tendo operations return `pipz.Chainable[*Tensor]`.

## Basics

### Running Operations

```go
import "github.com/zoobzio/pipz"

// Single operation
result, err := tendo.ReLU().Run(ctx, input)

// Operations take the input tensor via Run()
result, err := tendo.Add(other).Run(ctx, input)  // input + other
```

### Chaining Operations

```go
// Build a pipeline
forward := pipz.Chain(
    tendo.MatMul(weights1),
    tendo.Add(bias1),
    tendo.ReLU(),
    tendo.MatMul(weights2),
    tendo.Add(bias2),
    tendo.Softmax(-1),
)

// Run the pipeline
output, err := forward.Run(ctx, input)
if err != nil {
    log.Fatal(err)
}
```

## Common Chainable Operations

### Element-wise

```go
tendo.Add(other)        // Add tensor
tendo.Sub(other)        // Subtract tensor
tendo.Mul(other)        // Multiply tensor
tendo.Div(other)        // Divide by tensor
tendo.AddScalar(s)      // Add scalar
tendo.MulScalar(s)      // Multiply by scalar
tendo.Scale(s)          // Alias for MulScalar
tendo.Neg()             // Negate
tendo.Abs()             // Absolute value
tendo.Exp()             // Exponential
tendo.Log()             // Natural log
tendo.Sqrt()            // Square root
tendo.Square()          // Square
tendo.Pow(n)            // Power
```

### Matrix

```go
tendo.MatMul(other)         // Matrix multiplication
tendo.BatchedMatMul(other)  // Alias for MatMul
tendo.Transpose(d0, d1)     // Transpose dimensions
tendo.T()                   // Transpose last two dims
```

### Shape

```go
tendo.Reshape(shape...)         // Reshape
tendo.View(shape...)            // View (requires contiguous)
tendo.Flatten(startDim, endDim) // Flatten dimension range
tendo.Squeeze(dim...)           // Remove dimension(s)
tendo.Unsqueeze(dim)            // Add dimension
tendo.Slice(dim, start, end)    // Slice along dimension
tendo.Narrow(dim, start, len)   // Narrow along dimension
tendo.Expand(shape...)          // Broadcast expand
tendo.Permute(dims...)          // Reorder dimensions
```

### Reduction

```go
tendo.Sum(dims...)      // Sum along dimensions
tendo.Mean(dims...)     // Mean along dimensions
tendo.Max(dim)          // Max along dimension
tendo.Min(dim)          // Min along dimension
tendo.ArgMax(dim)       // Argmax along dimension
tendo.ArgMin(dim)       // Argmin along dimension
```

### Activation

```go
tendo.ReLU()                // ReLU
tendo.Sigmoid()             // Sigmoid
tendo.Tanh()                // Tanh
tendo.GELU()                // GELU
tendo.SiLU()                // SiLU (Swish)
tendo.LeakyReLU(alpha)      // Leaky ReLU
tendo.Softmax(dim)          // Softmax
tendo.LogSoftmax(dim)       // Log softmax
tendo.Dropout(p)            // Dropout
```

### Device & Type

```go
tendo.To(device)        // Move to device
tendo.ToCPU()           // Move to CPU
tendo.ToGPU(index)      // Move to GPU
tendo.MakeContiguous()  // Make contiguous
tendo.Sync()            // Synchronize device

tendo.ToFloat32()       // Convert to float32
tendo.ToFloat16()       // Convert to float16
tendo.ToBFloat16()      // Convert to bfloat16
tendo.ToDType(dtype)    // Convert to dtype
tendo.Float()           // Alias for ToFloat32
tendo.Half()            // Alias for ToFloat16
tendo.BFloat()          // Alias for ToBFloat16
```

## Building Layers

### Linear Layer

```go
func Linear(weights, bias *tendo.Tensor) pipz.Chainable[*tendo.Tensor] {
    return pipz.Chain(
        tendo.MatMul(weights),
        tendo.Add(bias),
    )
}

// Usage
layer := Linear(w, b)
output, _ := layer.Run(ctx, input)
```

### MLP Block

```go
func MLP(w1, b1, w2, b2 *tendo.Tensor) pipz.Chainable[*tendo.Tensor] {
    return pipz.Chain(
        Linear(w1, b1),
        tendo.ReLU(),
        Linear(w2, b2),
    )
}
```

### Residual Connection

```go
func Residual(block pipz.Chainable[*tendo.Tensor]) pipz.Chainable[*tendo.Tensor] {
    return pipz.Apply("residual", func(ctx context.Context, x *tendo.Tensor) (*tendo.Tensor, error) {
        out, err := block.Run(ctx, x)
        if err != nil {
            return nil, err
        }
        return tendo.Add(out).Run(ctx, x)
    })
}

// Usage
resBlock := Residual(pipz.Chain(
    Linear(w1, b1),
    tendo.ReLU(),
    Linear(w2, b2),
))
```

## Context and Inference Mode

### Inference vs Training Mode

```go
// Inference mode (default): dropout passes through
ctx := context.Background()
output, _ := network.Run(ctx, input)

// Training mode: dropout applies (for testing/debugging)
trainCtx := tendo.WithTraining(context.Background())
output, _ := network.Run(trainCtx, input)

// Check mode in custom ops
func customOp(ctx context.Context, t *tendo.Tensor) *tendo.Tensor {
    if tendo.IsTraining(ctx) {
        // Training behavior (dropout active, etc.)
    } else {
        // Inference behavior (default)
    }
}
```

### Dropout Example

```go
forward := pipz.Chain(
    Linear(w1, b1),
    tendo.ReLU(),
    tendo.Dropout(0.5),  // 50% dropout when in training mode
    Linear(w2, b2),
)

// Inference: dropout bypassed (default)
inferOut, _ := forward.Run(ctx, input)

// Training mode: dropout active
trainOut, _ := forward.Run(tendo.WithTraining(ctx), input)
```

## Error Handling

### Pipeline Errors

Errors propagate through the chain:

```go
forward := pipz.Chain(
    tendo.MatMul(weights),
    tendo.ReLU(),
)

output, err := forward.Run(ctx, input)
if err != nil {
    // Could be shape mismatch, device error, etc.
    var shapeErr *tendo.ShapeError
    if errors.As(err, &shapeErr) {
        log.Printf("Shape error: %v", err)
    }
}
```

### Custom Error Handling

```go
func SafeLayer(layer pipz.Chainable[*tendo.Tensor]) pipz.Chainable[*tendo.Tensor] {
    return pipz.Apply("safe_layer", func(ctx context.Context, t *tendo.Tensor) (*tendo.Tensor, error) {
        out, err := layer.Run(ctx, t)
        if err != nil {
            // Log and return input unchanged
            log.Printf("Layer failed: %v", err)
            return t, nil
        }
        return out, nil
    })
}
```

## Neural Network Example

### Simple Classifier

```go
type Classifier struct {
    forward pipz.Chainable[*tendo.Tensor]
}

func NewClassifier(inputDim, hiddenDim, outputDim int) *Classifier {
    // Initialize weights
    w1 := tendo.RandN(inputDim, hiddenDim)
    b1 := tendo.Zeros(hiddenDim)
    w2 := tendo.RandN(hiddenDim, outputDim)
    b2 := tendo.Zeros(outputDim)

    // Scale weights (Xavier initialization)
    scale1 := float32(math.Sqrt(2.0 / float64(inputDim)))
    scale2 := float32(math.Sqrt(2.0 / float64(hiddenDim)))
    w1, _ = tendo.MulScalar(scale1).Run(context.Background(), w1)
    w2, _ = tendo.MulScalar(scale2).Run(context.Background(), w2)

    return &Classifier{
        forward: pipz.Chain(
            tendo.MatMul(w1),
            tendo.Add(b1),
            tendo.ReLU(),
            tendo.Dropout(0.5),
            tendo.MatMul(w2),
            tendo.Add(b2),
            tendo.LogSoftmax(-1),
        ),
    }
}

func (c *Classifier) Forward(ctx context.Context, x *tendo.Tensor) (*tendo.Tensor, error) {
    return c.forward.Run(ctx, x)
}

// Usage
model := NewClassifier(784, 256, 10)

// Inference (default)
ctx := context.Background()
predictions, err := model.Forward(ctx, input)
```

### Convolutional Block

```go
func ConvBlock(weight *tendo.Tensor, config tendo.Conv2dConfig) pipz.Chainable[*tendo.Tensor] {
    return pipz.Chain(
        tendo.Conv2d(weight, config),
        tendo.ReLU(),
    )
}

// Usage
config := tendo.Conv2dConfig{
    Padding:  [2]int{1, 1},
    Stride:   [2]int{1, 1},
    Dilation: [2]int{1, 1},
    Groups:   1,
}
weight := tendo.RandN(32, 3, 3, 3)
block := ConvBlock(weight, config)
```

## Advanced Patterns

### Parallel Branches

```go
func ParallelBranches(branches ...pipz.Chainable[*tendo.Tensor]) pipz.Chainable[*tendo.Tensor] {
    return pipz.Apply("parallel", func(ctx context.Context, x *tendo.Tensor) (*tendo.Tensor, error) {
        results := make([]*tendo.Tensor, len(branches))

        for i, branch := range branches {
            out, err := branch.Run(ctx, x)
            if err != nil {
                return nil, err
            }
            results[i] = out
        }

        // Sum results (concatenation would require Cat operation)
        result := results[0]
        for i := 1; i < len(results); i++ {
            var err error
            result, err = tendo.Add(results[i]).Run(ctx, result)
            if err != nil {
                return nil, err
            }
        }
        return result, nil
    })
}
```

### Conditional Execution

```go
func Conditional(cond func(*tendo.Tensor) bool,
    thenBranch, elseBranch pipz.Chainable[*tendo.Tensor]) pipz.Chainable[*tendo.Tensor] {

    return pipz.Apply("conditional", func(ctx context.Context, x *tendo.Tensor) (*tendo.Tensor, error) {
        if cond(x) {
            return thenBranch.Run(ctx, x)
        }
        return elseBranch.Run(ctx, x)
    })
}
```

### Loop/Recurrence

```go
func Repeat(n int, block pipz.Chainable[*tendo.Tensor]) pipz.Chainable[*tendo.Tensor] {
    return pipz.Apply("repeat", func(ctx context.Context, x *tendo.Tensor) (*tendo.Tensor, error) {
        current := x
        var err error
        for i := 0; i < n; i++ {
            current, err = block.Run(ctx, current)
            if err != nil {
                return nil, err
            }
        }
        return current, nil
    })
}
```

### Custom Operations

```go
// Using pipz.Apply for operations that can fail
customOp := pipz.Apply("my_custom_op", func(ctx context.Context, t *tendo.Tensor) (*tendo.Tensor, error) {
    // Your custom logic
    if someCondition {
        return nil, errors.New("operation failed")
    }
    return result, nil
})

// Using pipz.Transform for operations that cannot fail
customTransform := pipz.Transform("my_transform", func(ctx context.Context, t *tendo.Tensor) *tendo.Tensor {
    // Your transform logic
    return result
})
```
