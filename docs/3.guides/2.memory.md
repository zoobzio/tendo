---
title: Memory Management
description: Memory pooling, allocation strategies, and optimization
author: zoobzio
published: 2025-12-16
updated: 2025-12-16
tags:
  - Memory
  - Pool
  - Optimization
---

# Memory Management

Efficient memory management is critical for tensor workloads. This guide covers tendo's memory pooling and optimization strategies.

## Memory Lifecycle

### CPU Memory

CPU storage uses Go slices:

```go
type CPUStorage struct {
    data  []float32
    dtype DType
}
```

Go's garbage collector handles CPU memory. Call `Free()` to release early:

```go
t := tendo.Zeros(1000, 1000)
// ... use tensor ...
t.Storage().Free()  // Release reference early
```

### CUDA Memory

CUDA storage requires explicit management:

```go
type CUDAStorage struct {
    ptr    uintptr    // Device pointer
    size   int        // Bytes
    len    int        // Elements
    device int        // GPU index
    dtype  DType
    pool   *Pool      // Optional pool
}
```

Without pooling, each allocation calls `cudaMalloc` and `cudaFree`:

```go
storage, _ := tendo.NewCUDAStorage(1000, tendo.Float32, 0)
// ... use ...
storage.Free()  // Calls cudaFree
```

## Memory Pool

The pool reduces allocation overhead by reusing freed memory.

### Creating a Pool

```go
pool := tendo.NewPool()
```

### Default Pool

A default pool is available globally:

```go
pool := tendo.DefaultPool()

// Replace default
customPool := tendo.NewPool()
tendo.SetDefaultPool(customPool)
```

### Pool Allocation

```go
pool := tendo.NewPool()

// Allocate CPU storage from pool
cpuStorage := pool.AllocCPU(1000, tendo.Float32)

// Allocate CUDA storage from pool
cudaPtr, err := pool.AllocCUDA(1000, tendo.Float32, 0)
```

### Returning to Pool

```go
// CPU: return to pool
pool.FreeCPU(cpuStorage)

// CUDA: return to pool
pool.FreeCUDA(cudaPtr, 1000, tendo.Float32, 0)
```

### Pool-Backed Storage

Create CUDA storage that automatically returns to pool:

```go
storage, err := tendo.NewCUDAStorageFromPool(1000, tendo.Float32, 0, pool)
if err != nil {
    log.Fatal(err)
}

// When freed, returns to pool instead of cudaFree
storage.Free()
```

## Size Classes

The pool uses size classes (powers of 2) for efficient matching:

```
Requested: 1000 elements
Size class: 1024 (next power of 2)

Requested: 2000 elements
Size class: 2048
```

This trades memory for faster allocation:

```go
// 1000 elements requested, 1024 allocated
// 24 elements wasted, but faster reuse
```

### Size Class Calculation

```go
func roundUpPow2(n int) int {
    n--
    n |= n >> 1
    n |= n >> 2
    n |= n >> 4
    n |= n >> 8
    n |= n >> 16
    n++
    return n
}
```

## Pool Statistics

Monitor memory usage:

```go
stats := pool.Stats()

// CPU stats
fmt.Printf("CPU allocations: %d\n", stats.CPUAllocations)
fmt.Printf("CPU deallocations: %d\n", stats.CPUDeallocations)
fmt.Printf("CPU bytes in use: %d\n", stats.CPUBytesInUse)
fmt.Printf("CPU bytes cached: %d\n", stats.CPUBytesCached)

// Per-GPU stats
for device, bytes := range stats.CUDABytesInUse {
    fmt.Printf("GPU %d bytes in use: %d\n", device, bytes)
}
for device, bytes := range stats.CUDABytesCached {
    fmt.Printf("GPU %d bytes cached: %d\n", device, bytes)
}
```

### Monitoring Example

```go
func logPoolStats(pool *tendo.Pool) {
    stats := pool.Stats()

    log.Printf("Pool stats:")
    log.Printf("  CPU: %d allocs, %d deallocs, %d bytes used, %d cached",
        stats.CPUAllocations,
        stats.CPUDeallocations,
        stats.CPUBytesInUse,
        stats.CPUBytesCached,
    )

    for device := range stats.CUDAAllocations {
        log.Printf("  GPU %d: %d allocs, %d deallocs, %d bytes used, %d cached",
            device,
            stats.CUDAAllocations[device],
            stats.CUDADeallocations[device],
            stats.CUDABytesInUse[device],
            stats.CUDABytesCached[device],
        )
    }
}
```

## Clearing the Pool

Release all cached memory:

```go
pool.Clear()
```

This:
- Clears CPU cache (GC handles actual freeing)
- Calls `cudaFree` on all cached CUDA blocks
- Resets cache statistics

Use `Clear()` when:
- Switching to a different workload
- Memory pressure is high
- Before measuring peak memory usage

## Pool Events

Pool operations emit capitan signals:

```go
// Track allocations
capitan.Hook(tendo.PoolAlloc, func(ctx context.Context, e *capitan.Event) {
    bytes, _ := tendo.KeyBytes.From(e)
    device, _ := tendo.KeyDevice.From(e)
    log.Printf("Allocated %d bytes on %s", bytes, device)
})

// Track frees
capitan.Hook(tendo.PoolFree, func(ctx context.Context, e *capitan.Event) {
    bytes, _ := tendo.KeyBytes.From(e)
    device, _ := tendo.KeyDevice.From(e)
    log.Printf("Freed %d bytes on %s", bytes, device)
})
```

### Event Context

Set pool context for event correlation:

```go
pool := tendo.NewPool()
pool.SetContext(ctx)  // Events will use this context
```

## Memory Optimization

### Reuse Tensors

Avoid creating new tensors in loops:

```go
ctx := context.Background()

// Bad: allocates each iteration
for i := 0; i < 1000; i++ {
    temp := tendo.Zeros(100, 100)
    result, _ := tendo.Add(temp).Run(ctx, a)
}

// Good: reuse buffer
temp := tendo.Zeros(100, 100)
for i := 0; i < 1000; i++ {
    // Reuse temp tensor
    result, _ := tendo.Add(temp).Run(ctx, a)
}
```

### Use Views When Possible

Views share storage:

```go
ctx := context.Background()

// Creates new storage
sliced := t.Clone()

// Shares storage (no allocation)
view, _ := tendo.Reshape(newShape...).Run(ctx, t)
```

### Batch Operations

Process data in batches to control memory:

```go
func processBatched(ctx context.Context, data *tendo.Tensor, batchSize int) ([]*tendo.Tensor, error) {
    n := data.Size(0)
    results := make([]*tendo.Tensor, 0)

    for i := 0; i < n; i += batchSize {
        end := min(i+batchSize, n)
        batch, err := tendo.Slice(0, i, end).Run(ctx, data)
        if err != nil {
            return nil, err
        }
        result, err := process(ctx, batch)
        if err != nil {
            return nil, err
        }
        results = append(results, result)
    }

    return results, nil
}
```

### Profile Memory Usage

```go
func profileMemory(fn func()) {
    pool := tendo.DefaultPool()

    // Clear and get baseline
    pool.Clear()
    before := pool.Stats()

    // Run workload
    fn()

    // Measure
    after := pool.Stats()

    fmt.Printf("CPU allocated: %d bytes\n",
        after.CPUBytesInUse - before.CPUBytesInUse)

    for device := range after.CUDABytesInUse {
        fmt.Printf("GPU %d allocated: %d bytes\n",
            device,
            after.CUDABytesInUse[device] - before.CUDABytesInUse[device])
    }
}
```

## Common Patterns

### Workspace Allocation

Pre-allocate workspace for repeated operations:

```go
type Workspace struct {
    pool   *tendo.Pool
    temp1  *tendo.Tensor
    temp2  *tendo.Tensor
}

func NewWorkspace(shape []int) *Workspace {
    pool := tendo.NewPool()
    return &Workspace{
        pool:  pool,
        temp1: tendo.ZerosOn(tendo.CUDADevice(0), shape...),
        temp2: tendo.ZerosOn(tendo.CUDADevice(0), shape...),
    }
}

func (w *Workspace) Free() {
    w.temp1.Storage().Free()
    w.temp2.Storage().Free()
    w.pool.Clear()
}
```

### Scoped Memory

Clean up after a computation:

```go
func withCleanup(fn func()) {
    pool := tendo.NewPool()
    oldPool := tendo.DefaultPool()
    tendo.SetDefaultPool(pool)

    defer func() {
        tendo.SetDefaultPool(oldPool)
        pool.Clear()
    }()

    fn()
}
```

### Memory Budget

Enforce memory limits:

```go
func checkMemoryBudget(pool *tendo.Pool, maxBytes int64) error {
    stats := pool.Stats()

    total := stats.CPUBytesInUse
    for _, bytes := range stats.CUDABytesInUse {
        total += bytes
    }

    if total > maxBytes {
        return fmt.Errorf("memory budget exceeded: %d > %d", total, maxBytes)
    }
    return nil
}
```
