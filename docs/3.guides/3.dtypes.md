---
title: Data Types
description: Float32, Float16, BFloat16 - precision, memory, and tradeoffs
author: zoobzio
published: 2025-12-16
updated: 2025-12-16
tags:
  - DType
  - Float16
  - BFloat16
  - Precision
---

# Data Types

Tendo supports three floating-point data types with different precision and memory tradeoffs.

## Available Types

| Type | Bits | Exponent | Mantissa | Range | Use Case |
|------|------|----------|----------|-------|----------|
| Float32 | 32 | 8 | 23 | ~1e-38 to ~3e38 | General computation |
| Float16 | 16 | 5 | 10 | ~6e-5 to ~65504 | Memory-constrained inference |
| BFloat16 | 16 | 8 | 7 | ~1e-38 to ~3e38 | Training with reduced memory |

## Float32 (Default)

Standard 32-bit IEEE 754 single precision:

```
┌───┬──────────┬───────────────────────┐
│ S │ Exponent │       Mantissa        │
│ 1 │    8     │          23           │
└───┴──────────┴───────────────────────┘
```

- Full precision for most applications
- Native hardware support on all devices
- Default dtype in tendo

```go
t := tendo.Zeros(100, 100)  // Float32 by default
t.DType()  // tendo.Float32
```

## Float16 (IEEE 754 Half)

16-bit IEEE 754 half precision:

```
┌───┬──────────┬──────────┐
│ S │ Exponent │ Mantissa │
│ 1 │    5     │    10    │
└───┴──────────┴──────────┘
```

- Half the memory of Float32
- Reduced precision (3-4 decimal digits)
- Limited range: values outside ~65504 overflow to infinity

### Use Cases

- Inference where precision is less critical
- Large models that don't fit in memory
- Mobile/embedded deployment

```go
tendo.SetDefaultDType(tendo.Float16)
t := tendo.Zeros(100, 100)  // Float16
```

## BFloat16 (Brain Float)

16-bit with Float32's exponent range:

```
┌───┬──────────┬─────────┐
│ S │ Exponent │ Mantissa│
│ 1 │    8     │    7    │
└───┴──────────┴─────────┘
```

- Same range as Float32
- Lower precision than Float16 (2-3 decimal digits)
- Better for training: maintains gradient magnitudes

### Use Cases

- Training with reduced memory
- When dynamic range matters more than precision
- Mixed-precision training

```go
tendo.SetDefaultDType(tendo.BFloat16)
t := tendo.Zeros(100, 100)  // BFloat16
```

## Setting Default DType

```go
// Get current default
dtype := tendo.DefaultDType()  // Initially Float32

// Set default
tendo.SetDefaultDType(tendo.Float16)

// All new tensors use Float16
t := tendo.Zeros(100, 100)
t.DType()  // tendo.Float16
```

## DType Properties

```go
dtype := tendo.Float16

dtype.String()  // "float16"
dtype.Size()    // 2 (bytes per element)

// Memory calculation
numel := 1000000
memBytes := numel * dtype.Size()  // 2,000,000 bytes
```

## Type Conversion

### Conversion Functions

All type conversions return `pipz.Chainable[*Tensor]`:

```go
ctx := context.Background()

// To Float16
f16, _ := tendo.ToFloat16().Run(ctx, tensor)

// To Float32
f32, _ := tendo.ToFloat32().Run(ctx, tensor)

// To BFloat16
bf16, _ := tendo.ToBFloat16().Run(ctx, tensor)

// Using aliases
half, _ := tendo.Half().Run(ctx, tensor)    // ToFloat16
float, _ := tendo.Float().Run(ctx, tensor)  // ToFloat32
bfloat, _ := tendo.BFloat().Run(ctx, tensor) // ToBFloat16

// Generic conversion
converted, _ := tendo.ToDType(tendo.Float16).Run(ctx, tensor)
```

### Manual Conversion

Low-level conversion utilities:

```go
// Single value
f32 := float32(3.14159)
f16 := tendo.Float32ToFloat16(f32)   // Returns uint16
back := tendo.Float16ToFloat32(f16)  // Returns float32

// BFloat16
bf16 := tendo.Float32ToBFloat16(f32)
back := tendo.BFloat16ToFloat32(bf16)
```

### Slice Conversion

```go
data := []float32{1.0, 2.0, 3.0, 4.0}

// To Float16
f16Data := tendo.Float32SliceToFloat16(data)  // []uint16

// Back to Float32
f32Data := tendo.Float16SliceToFloat32(f16Data)  // []float32

// BFloat16
bf16Data := tendo.Float32SliceToBFloat16(data)
f32Data := tendo.BFloat16SliceToFloat32(bf16Data)
```

## Precision Comparison

### Float32 vs Float16

```go
// Float32: full precision
f32 := float32(3.141592653589793)
// Stored as: 3.1415927

// Float16: reduced precision
f16 := tendo.Float32ToFloat16(f32)
back := tendo.Float16ToFloat32(f16)
// Stored as: 3.140625 (loses precision)
```

### Float32 vs BFloat16

```go
// Float32
f32 := float32(3.141592653589793)

// BFloat16: even less precision, same range
bf16 := tendo.Float32ToBFloat16(f32)
back := tendo.BFloat16ToFloat32(bf16)
// Stored as: 3.140625 (truncated mantissa)
```

### Range Differences

```go
// Float32 and BFloat16: same range
large := float32(1e38)
bf16 := tendo.Float32ToBFloat16(large)  // OK

// Float16: limited range
f16 := tendo.Float32ToFloat16(large)
// Overflows to infinity
```

## Internal Storage

CPU storage always uses float32 internally for simplicity:

```go
// From storage_cpu.go
type CPUStorage struct {
    data  []float32  // Always float32
    dtype DType      // Tracks logical dtype
}
```

Conversion happens on device transfer:

```go
ctx := context.Background()

// CPU -> CUDA with Float16
cpuTensor := tendo.Zeros(100, 100)
tendo.SetDefaultDType(tendo.Float16)

// Transfer converts to actual Float16 on GPU
gpuTensor, _ := tendo.ToGPU(0).Run(ctx, cpuTensor)
```

This design:
- Simplifies CPU operations (no Float16 arithmetic)
- Provides memory savings on GPU
- Handles conversion automatically on transfer

## Best Practices

### Inference

Use Float16 for deployed models:

```go
func loadModelForInference(ctx context.Context) {
    tendo.SetDefaultDType(tendo.Float16)

    // Load weights (stored as Float16)
    weights := loadWeights()

    // Run inference
    output, _ := model.Forward(ctx, input)
}
```

### Training

Use BFloat16 for training when memory-constrained:

```go
func train(ctx context.Context) {
    tendo.SetDefaultDType(tendo.BFloat16)

    // Gradients maintain dynamic range
    for batch := range dataloader {
        loss, _ := forward(ctx, batch)
        backward(ctx, loss)  // BFloat16 gradients
    }
}
```

### Mixed Precision

Keep some operations in Float32:

```go
ctx := context.Background()

// Weights in reduced precision
tendo.SetDefaultDType(tendo.Float16)
weights := loadWeights()

// Loss computation in Float32 for stability
f32Output, _ := tendo.ToFloat32().Run(ctx, output)
f32Target, _ := tendo.ToFloat32().Run(ctx, target)
loss, _ := computeLoss(ctx, f32Output, f32Target)
```

### Precision-Sensitive Operations

Some operations need Float32:

```go
ctx := context.Background()

// Softmax with Float16 can overflow
// Compute in Float32, convert back
f32Input, _ := tendo.ToFloat32().Run(ctx, input)
f32Output, _ := tendo.Softmax(-1).Run(ctx, f32Input)
output, _ := tendo.ToFloat16().Run(ctx, f32Output)
```

## Memory Savings

| Dtype | 1M Elements | 100M Elements |
|-------|-------------|---------------|
| Float32 | 4 MB | 400 MB |
| Float16 | 2 MB | 200 MB |
| BFloat16 | 2 MB | 200 MB |

For large models, Float16/BFloat16 can:
- Fit larger batch sizes
- Enable larger models
- Reduce memory transfer time
