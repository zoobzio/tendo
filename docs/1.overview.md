---
title: Overview
description: Composable tensor library for Go with GPU acceleration
author: zoobzio
published: 2025-12-16
updated: 2025-12-16
tags:
  - Overview
  - Introduction
---

# Overview

Tensor libraries in Go often mean heavyweight C++ bindings or limited pure-Go implementations.

Tendo offers a different path: composable tensor operations built on Go idioms, with optional GPU acceleration.

```go
ctx := context.Background()

// Create tensors
a := tendo.RandN(128, 64)
b := tendo.RandN(64, 32)

// Operations return pipz.Chainable
c, _ := tendo.MatMul(b).Run(ctx, a)
d, _ := tendo.ReLU().Run(ctx, c)
e, _ := tendo.Softmax(-1).Run(ctx, d)

// Compose as pipelines
forward := pipz.Chain(
    tendo.MatMul(weights),
    tendo.Add(bias),
    tendo.ReLU(),
)
result, _ := forward.Run(ctx, input)
```

Type-safe, composable, observable.

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                           Tendo                                  │
│                                                                  │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │                      Tensor                                 │  │
│  │         shape + stride + offset + Storage                   │  │
│  └────────────────────────────────────────────────────────────┘  │
│                              │                                   │
│              ┌───────────────┴───────────────┐                   │
│              ▼                               ▼                   │
│  ┌─────────────────────┐         ┌─────────────────────┐        │
│  │    CPUStorage       │         │    CUDAStorage      │        │
│  │    []float32        │         │    device pointer   │        │
│  │    Go memory        │         │    GPU memory       │        │
│  └─────────────────────┘         └─────────────────────┘        │
│                                                                  │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │                    Operations                               │  │
│  │  Elementwise │ Matrix │ Shape │ Reduce │ Activation │ Conv  │  │
│  └────────────────────────────────────────────────────────────┘  │
│                                                                  │
│  ┌─────────────────────┐         ┌─────────────────────┐        │
│  │       Pool          │         │      Signals        │        │
│  │   Memory reuse      │         │  capitan events     │        │
│  └─────────────────────┘         └─────────────────────┘        │
└─────────────────────────────────────────────────────────────────┘
```

Tensors wrap a Storage backend (CPU or CUDA) with shape metadata. Operations dispatch to the appropriate device. Events flow through capitan for observability and autograd integration.

## Philosophy

Tendo draws inspiration from PyTorch's tensor API while embracing Go's strengths: explicit error handling, composition over inheritance, and clean interfaces.

```go
// Define a simple neural network layer
func Linear(weights, bias *tendo.Tensor) pipz.Chainable[*tendo.Tensor] {
    return pipz.Chain(
        tendo.MatMul(weights),
        tendo.Add(bias),
    )
}

// Stack layers into a network
network := pipz.Chain(
    Linear(w1, b1),
    tendo.ReLU(),
    Linear(w2, b2),
    tendo.Softmax(-1),
)

// Run inference
output, err := network.Run(ctx, input)
```

Operations compose via [pipz](https://github.com/zoobzio/pipz). Events emit via [capitan](https://github.com/zoobzio/capitan). The pieces fit together without tight coupling.

## Capabilities

A unified tensor interface enables:

**[Device Portability](3.guides/1.devices.md)** - Write operations once, run on CPU or CUDA. Transfer tensors between devices with a single call.

**[Memory Efficiency](3.guides/2.memory.md)** - Pool allocations to reduce GC pressure and CUDA malloc overhead. Track memory usage through pool statistics.

**[Multiple Precisions](3.guides/3.dtypes.md)** - Float32 for accuracy, Float16/BFloat16 for memory efficiency. Automatic conversion on device transfer.

**[Observability](4.cookbook/2.observability.md)** - Every operation emits capitan signals. Hook listeners for logging, profiling, or building autograd systems.

**[Composition](4.cookbook/1.pipelines.md)** - Build computation graphs as pipz pipelines. Chain operations, handle errors, compose layers.

## Priorities

### Composability

Operations return `pipz.Chainable` for building computation graphs:

```go
// Chainable operations
forward := pipz.Chain(
    tendo.MatMul(weights),
    tendo.ReLU(),
    tendo.Dropout(0.5),
)

// Context controls behavior
ctx := tendo.WithTraining(context.Background())
output, err := forward.Run(ctx, input)
```

Training vs inference behavior is context-driven. Dropout applies during training, passes through during inference.

### Device Abstraction

The Storage interface abstracts memory location:

```go
type Storage interface {
    Ptr() uintptr
    Size() int
    Len() int
    Device() Device
    DType() DType
    Clone() Storage
    Free()
}
```

Operations check device compatibility and dispatch accordingly. Device mismatches are caught at runtime with clear errors.

### Observability

Every operation emits signals:

```go
// Hook operation events
capitan.Hook(tendo.OpMatMul, func(ctx context.Context, e *capitan.Event) {
    input, _ := tendo.KeyInputA.From(e)
    output, _ := tendo.KeyOutput.From(e)
    log.Printf("MatMul: %v -> %v", input.Shape(), output.Shape())
})
```

Signals provide the foundation for:
- Operation logging and profiling
- Computation graph capture
- Autograd tape recording

### Performance

- **Lazy initialization** - CUDA contexts created on first use
- **Memory pooling** - Reduce allocation overhead
- **Contiguous optimization** - Fast paths for contiguous tensors
- **cuBLAS integration** - Hardware-accelerated matrix operations
